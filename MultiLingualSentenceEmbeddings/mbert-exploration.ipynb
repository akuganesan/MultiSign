{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ix-P_5mFezG",
        "outputId": "cd993edf-5400-4c17-d58b-41c8c1bac5f4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPuT376uF67l"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizers (vocabulary)\n",
        "mbert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "msbert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fT0KHwHUEZ"
      },
      "source": [
        "# Load pre-trained models (weights)\n",
        "mbert = BertModel.from_pretrained('bert-base-multilingual-uncased',\n",
        "                                  output_hidden_states = True,\n",
        "                                  )\n",
        "msbert = BertModel.from_pretrained('DeepPavlov/bert-base-multilingual-cased-sentence',\n",
        "                                  output_hidden_states = True,\n",
        "                                  )"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOwZisRljFYy",
        "outputId": "b4595902-f82a-44fb-c681-f4c8c5b98deb"
      },
      "source": [
        "mbert.eval(); msbert.eval()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBBTyKawXM6F"
      },
      "source": [
        "## Toy example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C22kZH5tGRS3",
        "outputId": "3f20744a-b3b8-412d-ee23-f1ddcf71faa8"
      },
      "source": [
        "# This is a car\n",
        "text1 = \"Това е кола.\"\n",
        "text2 = \"Dies ist ein Auto.\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "mbert_tokenized_text1 = mbert_tokenizer(text1, padding=True, return_tensors=\"pt\")\n",
        "mbert_tokenized_text2 = mbert_tokenizer(text2, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Print out the tokens.\n",
        "print(mbert_tokenized_text1)\n",
        "print(mbert_tokenized_text2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101, 15036,   312, 63896,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
            "{'input_ids': tensor([[  101, 13015, 10339, 10299, 14929,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_3QYzTNHaHa"
      },
      "source": [
        "with torch.no_grad():\n",
        "    outputs1 = mbert(**mbert_tokenized_text1)\n",
        "    embedding_1 = outputs1.pooler_output\n",
        "\n",
        "    outputs2 = mbert(**mbert_tokenized_text2)\n",
        "    embedding_2 = outputs2.pooler_output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm3izoQtLuLY",
        "outputId": "fa380644-c043-4748-dd07-b0f92051c8ad"
      },
      "source": [
        "cosine_sim = torch.nn.CosineSimilarity() \n",
        "cosine_sim(embedding_1, embedding_2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9897])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6iS2djCaLdQ",
        "outputId": "42b56888-7d1d-4547-daba-309e4caa0fc5"
      },
      "source": [
        "# This is a car\n",
        "text1 = \"Това е кола.\"\n",
        "text2 = \"Dies ist ein Auto.\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "msbert_tokenized_text1 = msbert_tokenizer(text1, padding=True, return_tensors=\"pt\")\n",
        "msbert_tokenized_text2 = msbert_tokenizer(text2, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Print out the tokens.\n",
        "print(msbert_tokenized_text1)\n",
        "print(msbert_tokenized_text2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101, 36231,   546, 79123,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
            "{'input_ids': tensor([[  101, 18231, 10298, 10290, 23265,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO-YMkW1aQg9"
      },
      "source": [
        "with torch.no_grad():\n",
        "    outputs1 = msbert(**msbert_tokenized_text1)\n",
        "    embedding_1 = outputs1.pooler_output\n",
        "\n",
        "    outputs2 = msbert(**msbert_tokenized_text2)\n",
        "    embedding_2 = outputs2.pooler_output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhJTSjqaaWrz",
        "outputId": "2adeb4ff-952d-41ca-9070-ff00dc815a66"
      },
      "source": [
        "cosine_sim(embedding_1, embedding_2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5499])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0zXFKAQXRLO"
      },
      "source": [
        "## Testing on dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE2a7WKEcXMc",
        "outputId": "3aa3d5fb-621b-4ecc-cc05-e7cd4a31f1bf"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4QWx-5Scoo8",
        "outputId": "e0a3b7e1-572d-48a9-d973-6cb3cf7a6ab9"
      },
      "source": [
        "cd './drive/My Drive/Colab Notebooks/CPSC 532S Project/'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/CPSC 532S Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I9o0WKSb2AK"
      },
      "source": [
        "data = pd.read_csv(\"data/xnli.15way.orig.tsv\", delimiter=\"\\t\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_-VwmlPfDWP"
      },
      "source": [
        "bg = data[\"bg\"]\n",
        "en = data[\"en\"]\n",
        "de = data[\"de\"]\n",
        "sw = data[\"sw\"]\n",
        "ar = data[\"ar\"]\n",
        "vi = data[\"vi\"]\n",
        "zh = data[\"zh\"]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaGsLadqfaH0",
        "outputId": "61b57637-19ef-4842-8b03-fd84f65f396f"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ps07fpPff1Q"
      },
      "source": [
        "### Multilingual Bert Base (uncased)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGFvmenAeQqZ",
        "outputId": "1cc70761-6033-4285-c217-47b88f2acf37"
      },
      "source": [
        "similarities = []\n",
        "\n",
        "for i in range(1000):\n",
        "   if i%100 == 0:\n",
        "     print(\"{}/1000\".format(i))\n",
        "   sentences = [bg[i]] + [en[i]] + [de[i]] + [sw[i]] + [ar[i]] + [vi[i]] + [zh[i]]\n",
        "   with torch.no_grad():\n",
        "      tokenized_sentences = mbert_tokenizer(sentences,\n",
        "                                            padding=True,\n",
        "                                            return_tensors=\"pt\")\n",
        "      embeddings = mbert(**tokenized_sentences).pooler_output\n",
        "\n",
        "   s = []\n",
        "   for n in range(embeddings.shape[0]):\n",
        "     for m in range(n+1, embeddings.shape[0]):\n",
        "       s.append(cosine_sim(embeddings[n:n+1], embeddings[m:m+1]))\n",
        "\n",
        "   similarities.append(s)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/1000\n",
            "100/1000\n",
            "200/1000\n",
            "300/1000\n",
            "400/1000\n",
            "500/1000\n",
            "600/1000\n",
            "700/1000\n",
            "800/1000\n",
            "900/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbjoeae2jnw7"
      },
      "source": [
        "similarities = torch.Tensor(similarities)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3KBNiqtiXIO"
      },
      "source": [
        "mean_sim = torch.mean(similarities, dim=0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCXHGhmxidJd",
        "outputId": "1ecf490c-3d90-40ea-c072-2e55f47e3767"
      },
      "source": [
        "languages = [\"Bulgarian -- English\", \"Bulgarian -- German\",\n",
        "             \"Bulgarian -- Swahili\", \"Bulgarian -- Arabic\",\n",
        "             \"Bulgarian -- Vietnamese\", \"Bulgarian -- Mandarin\",\n",
        "             \"English   -- German\", \"English   -- Swahili\", \n",
        "             \"English   -- Arabic\", \"English   -- Vietnamese\",\n",
        "             \"English   -- Mandarin\", \"German    -- Swahili\", \n",
        "             \"German    -- Arabic\", \"German    -- Vietnamese\",\n",
        "             \"German    -- Mandarin\", \"Swahili   -- Arabic\",\n",
        "             \"Swahili   -- Vietnamese\", \"Swahili   -- Mandarin\",\n",
        "             \"Arabic    -- Vietnamese\", \"Arabic    -- Mandarin\",\n",
        "             \"Vietnamese -- Mandarin\"]\n",
        "\n",
        "for i in range(len(languages)):\n",
        "  print(\"{:<30} mean similarity   {:.4f}\".format(languages[i], mean_sim[i]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bulgarian -- English           mean similarity   0.9464\n",
            "Bulgarian -- German            mean similarity   0.9647\n",
            "Bulgarian -- Swahili           mean similarity   0.9413\n",
            "Bulgarian -- Arabic            mean similarity   0.9661\n",
            "Bulgarian -- Vietnamese        mean similarity   0.9686\n",
            "Bulgarian -- Mandarin          mean similarity   0.8772\n",
            "English   -- German            mean similarity   0.9633\n",
            "English   -- Swahili           mean similarity   0.9046\n",
            "English   -- Arabic            mean similarity   0.9418\n",
            "English   -- Vietnamese        mean similarity   0.9395\n",
            "English   -- Mandarin          mean similarity   0.8149\n",
            "German    -- Swahili           mean similarity   0.9237\n",
            "German    -- Arabic            mean similarity   0.9576\n",
            "German    -- Vietnamese        mean similarity   0.9569\n",
            "German    -- Mandarin          mean similarity   0.8344\n",
            "Swahili   -- Arabic            mean similarity   0.9533\n",
            "Swahili   -- Vietnamese        mean similarity   0.9539\n",
            "Swahili   -- Mandarin          mean similarity   0.8940\n",
            "Arabic    -- Vietnamese        mean similarity   0.9697\n",
            "Arabic    -- Mandarin          mean similarity   0.8839\n",
            "Vietnamese -- Mandarin         mean similarity   0.8926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU4Ao96Cj4CO"
      },
      "source": [
        "### Multilingual Bert Sentence (cased)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8blLYpYmj3p4",
        "outputId": "211a0152-16f0-48e5-f231-4bd205bbed25"
      },
      "source": [
        "similarities = []\n",
        "\n",
        "for i in range(1000):\n",
        "   if i%100 == 0:\n",
        "     print(\"{}/1000\".format(i))\n",
        "   \n",
        "   sentences = [bg[i]] + [en[i]] + [de[i]] + [sw[i]] + [ar[i]] + [vi[i]] + [zh[i]]\n",
        "   with torch.no_grad():\n",
        "      tokenized_sentences = msbert_tokenizer(sentences,\n",
        "                                            padding=True,\n",
        "                                            return_tensors=\"pt\")\n",
        "      embeddings = msbert(**tokenized_sentences).pooler_output\n",
        "\n",
        "   s = []\n",
        "   for n in range(embeddings.shape[0]):\n",
        "     for m in range(n+1, embeddings.shape[0]):\n",
        "       s.append(cosine_sim(embeddings[n:n+1], embeddings[m:m+1]))\n",
        "\n",
        "   similarities.append(s)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/1000\n",
            "100/1000\n",
            "200/1000\n",
            "300/1000\n",
            "400/1000\n",
            "500/1000\n",
            "600/1000\n",
            "700/1000\n",
            "800/1000\n",
            "900/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAR61Y37j3nz"
      },
      "source": [
        "similarities = torch.Tensor(similarities)\n",
        "mean_sim = torch.mean(similarities, dim=0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L33ZshgYj3Ye",
        "outputId": "d76158a4-2411-4e0f-b75c-f773768e37db"
      },
      "source": [
        "for i in range(len(languages)):\n",
        "    print(\"{:<30} mean similarity   {:.4f}\".format(languages[i], mean_sim[i]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bulgarian -- English           mean similarity   0.9033\n",
            "Bulgarian -- German            mean similarity   0.9018\n",
            "Bulgarian -- Swahili           mean similarity   0.8193\n",
            "Bulgarian -- Arabic            mean similarity   0.8869\n",
            "Bulgarian -- Vietnamese        mean similarity   0.8924\n",
            "Bulgarian -- Mandarin          mean similarity   0.8837\n",
            "English   -- German            mean similarity   0.9166\n",
            "English   -- Swahili           mean similarity   0.7939\n",
            "English   -- Arabic            mean similarity   0.8779\n",
            "English   -- Vietnamese        mean similarity   0.9015\n",
            "English   -- Mandarin          mean similarity   0.9005\n",
            "German    -- Swahili           mean similarity   0.8083\n",
            "German    -- Arabic            mean similarity   0.8799\n",
            "German    -- Vietnamese        mean similarity   0.8925\n",
            "German    -- Mandarin          mean similarity   0.8865\n",
            "Swahili   -- Arabic            mean similarity   0.8164\n",
            "Swahili   -- Vietnamese        mean similarity   0.8045\n",
            "Swahili   -- Mandarin          mean similarity   0.7833\n",
            "Arabic    -- Vietnamese        mean similarity   0.8738\n",
            "Arabic    -- Mandarin          mean similarity   0.8649\n",
            "Vietnamese -- Mandarin         mean similarity   0.8914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID_d3wnhpHKL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}