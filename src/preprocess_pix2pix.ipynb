{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "statewide-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from utils.dataset import SIGNUMDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import utils.skel as skel\n",
    "import deepdish as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import torchvision\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "contemporary-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SIGNUMDataset('/scratch/datasets/SIGNUM', use_pose=True, subsample=10, normalize_poses=True, testing=True, speaker_id=\"11\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=train_dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accepting-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write images to a directory\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    data_dir = f'/scratch/datasets/pose2video/train/a/s11-p01-c{i+1:04d}-f'\n",
    "    img_seq = torch.FloatTensor(data['img_seq'])\n",
    "    pose_seq = data['pose_seq'].squeeze(0)\n",
    "    for p, pose in enumerate(pose_seq):\n",
    "        pose = skel.denormalize_pose(pose, train_dataset.mean, train_dataset.std)\n",
    "        ax = plt.subplot(111)\n",
    "        skel.plot_pose2D(ax, pose)\n",
    "        data_path = data_dir + f'{p+1:04d}.jpg'\n",
    "        plt.show()\n",
    "        plt.axis('off')\n",
    "        plt.savefig(data_path, bbox_inches='tight')\n",
    "        print(data_path)\n",
    "        ax.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "departmental-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write images to b directory\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    data_dir = f'/scratch/datasets/pose2video/test/b/s11-p01-c{i+1:04d}-f'\n",
    "    img_seq = torch.FloatTensor(data['img_seq'])\n",
    "    for p, img in enumerate(img_seq[0]):\n",
    "        save_image(img, data_dir + f'{p+1:04d}.jpg', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "crazy-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822 1900\n",
      "s11-p01-c0001-f0022.jpg\n",
      "s11-p01-c0002-f0022.jpg\n",
      "s11-p01-c0003-f0023.jpg\n",
      "s11-p01-c0004-f0018.jpg\n",
      "s11-p01-c0005-f0019.jpg\n",
      "s11-p01-c0006-f0017.jpg\n",
      "s11-p01-c0007-f0019.jpg\n",
      "s11-p01-c0008-f0028.jpg\n",
      "s11-p01-c0009-f0019.jpg\n",
      "s11-p01-c0010-f0019.jpg\n",
      "s11-p01-c0011-f0025.jpg\n",
      "s11-p01-c0012-f0025.jpg\n",
      "s11-p01-c0013-f0023.jpg\n",
      "s11-p01-c0014-f0018.jpg\n",
      "s11-p01-c0015-f0021.jpg\n",
      "s11-p01-c0016-f0020.jpg\n",
      "s11-p01-c0017-f0021.jpg\n",
      "s11-p01-c0018-f0021.jpg\n",
      "s11-p01-c0019-f0024.jpg\n",
      "s11-p01-c0020-f0027.jpg\n",
      "s11-p01-c0021-f0019.jpg\n",
      "s11-p01-c0022-f0035.jpg\n",
      "s11-p01-c0023-f0021.jpg\n",
      "s11-p01-c0024-f0025.jpg\n",
      "s11-p01-c0025-f0015.jpg\n",
      "s11-p01-c0026-f0024.jpg\n",
      "s11-p01-c0027-f0027.jpg\n",
      "s11-p01-c0028-f0028.jpg\n",
      "s11-p01-c0029-f0022.jpg\n",
      "s11-p01-c0030-f0020.jpg\n",
      "s11-p01-c0031-f0022.jpg\n",
      "s11-p01-c0032-f0023.jpg\n",
      "s11-p01-c0033-f0028.jpg\n",
      "s11-p01-c0034-f0029.jpg\n",
      "s11-p01-c0035-f0026.jpg\n",
      "s11-p01-c0036-f0023.jpg\n",
      "s11-p01-c0037-f0023.jpg\n",
      "s11-p01-c0038-f0022.jpg\n",
      "s11-p01-c0039-f0024.jpg\n",
      "s11-p01-c0040-f0023.jpg\n",
      "s11-p01-c0041-f0027.jpg\n",
      "s11-p01-c0042-f0021.jpg\n",
      "s11-p01-c0043-f0022.jpg\n",
      "s11-p01-c0044-f0025.jpg\n",
      "s11-p01-c0045-f0031.jpg\n",
      "s11-p01-c0046-f0023.jpg\n",
      "s11-p01-c0047-f0022.jpg\n",
      "s11-p01-c0048-f0024.jpg\n",
      "s11-p01-c0049-f0022.jpg\n",
      "s11-p01-c0050-f0023.jpg\n",
      "s11-p01-c0051-f0025.jpg\n",
      "s11-p01-c0052-f0026.jpg\n",
      "s11-p01-c0053-f0029.jpg\n",
      "s11-p01-c0054-f0024.jpg\n",
      "s11-p01-c0055-f0029.jpg\n",
      "s11-p01-c0056-f0022.jpg\n",
      "s11-p01-c0057-f0026.jpg\n",
      "s11-p01-c0058-f0023.jpg\n",
      "s11-p01-c0059-f0029.jpg\n",
      "s11-p01-c0060-f0028.jpg\n",
      "s11-p01-c0061-f0023.jpg\n",
      "s11-p01-c0062-f0027.jpg\n",
      "s11-p01-c0063-f0027.jpg\n",
      "s11-p01-c0064-f0027.jpg\n",
      "s11-p01-c0065-f0022.jpg\n",
      "s11-p01-c0066-f0027.jpg\n",
      "s11-p01-c0067-f0026.jpg\n",
      "s11-p01-c0068-f0025.jpg\n",
      "s11-p01-c0069-f0034.jpg\n",
      "s11-p01-c0070-f0025.jpg\n",
      "s11-p01-c0071-f0024.jpg\n",
      "s11-p01-c0072-f0032.jpg\n",
      "s11-p01-c0073-f0029.jpg\n",
      "s11-p01-c0074-f0028.jpg\n",
      "s11-p01-c0075-f0025.jpg\n",
      "s11-p01-c0076-f0028.jpg\n",
      "s11-p01-c0077-f0030.jpg\n",
      "s11-p01-c0078-f0030.jpg\n"
     ]
    }
   ],
   "source": [
    "# one-off for files so remove extras\n",
    "filesa = sorted(glob.glob(os.path.join('/scratch/datasets/pose2video/test/a/', '*.jpg')))\n",
    "filesb = sorted(glob.glob(os.path.join('/scratch/datasets/pose2video/test/b/', '*.jpg')))\n",
    "print(len(filesa), len(filesb))\n",
    "\n",
    "for file in filesb:\n",
    "    if '/scratch/datasets/pose2video/test/a/' + file.split('/')[6] not in filesa:\n",
    "        print(file.split('/')[6])\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "athletic-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822 1822\n"
     ]
    }
   ],
   "source": [
    "# ensure a and b are same length\n",
    "filesa = sorted(glob.glob(os.path.join('/scratch/datasets/pose2video/test/a/', '*.jpg')))\n",
    "filesb = sorted(glob.glob(os.path.join('/scratch/datasets/pose2video/test/b/', '*.jpg')))\n",
    "print(len(filesa), len(filesb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
