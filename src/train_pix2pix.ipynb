{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welsh-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from utils.dataset import SIGNUMDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import utils.skel as skel\n",
    "import deepdish as dd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "innocent-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, activation=True, batch_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.activation = activation\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2, True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation:\n",
    "            out = self.conv(self.lrelu(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            return self.bn(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class DeconvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, batch_norm=True, dropout=False):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "        self.drop = torch.nn.Dropout(0.5)\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(self.relu(x)))\n",
    "        else:\n",
    "            out = self.deconv(self.relu(x))\n",
    "\n",
    "        if self.dropout:\n",
    "            return self.drop(out)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv6 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv7 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv8 = ConvBlock(num_filter * 8, num_filter * 8, batch_norm=False)\n",
    "        # Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter * 8, num_filter * 8, dropout=True)\n",
    "        self.deconv2 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv3 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv4 = DeconvBlock(num_filter * 8 * 2, num_filter * 8)\n",
    "        self.deconv5 = DeconvBlock(num_filter * 8 * 2, num_filter * 4)\n",
    "        self.deconv6 = DeconvBlock(num_filter * 4 * 2, num_filter * 2)\n",
    "        self.deconv7 = DeconvBlock(num_filter * 2 * 2, num_filter)\n",
    "        self.deconv8 = DeconvBlock(num_filter * 2, output_dim, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(x)\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        enc4 = self.conv4(enc3)\n",
    "        enc5 = self.conv5(enc4)\n",
    "        enc6 = self.conv6(enc5)\n",
    "        enc7 = self.conv7(enc6)\n",
    "        enc8 = self.conv8(enc7)\n",
    "        # Decoder with skip-connections\n",
    "        dec1 = self.deconv1(enc8)\n",
    "        dec1 = torch.cat([dec1, enc7], 1)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc6], 1)\n",
    "        dec3 = self.deconv3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc5], 1)\n",
    "        dec4 = self.deconv4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc4], 1)\n",
    "        dec5 = self.deconv5(dec4)\n",
    "        dec5 = torch.cat([dec5, enc3], 1)\n",
    "        dec6 = self.deconv6(dec5)\n",
    "        dec6 = torch.cat([dec6, enc2], 1)\n",
    "        dec7 = self.deconv7(dec6)\n",
    "        dec7 = torch.cat([dec7, enc1], 1)\n",
    "        dec8 = self.deconv8(dec7)\n",
    "        out = torch.nn.Tanh()(dec8)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "            if isinstance(m, DeconvBlock):\n",
    "                torch.nn.init.normal(m.deconv.weight, mean, std)\n",
    "                \n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8, stride=1)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, output_dim, stride=1, batch_norm=False)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat([x, label], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        out = torch.nn.Sigmoid()(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ahead-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    img = img.resize((256, 256), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_img(image_tensor, filename):\n",
    "    image_numpy = image_tensor.float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    image_numpy = image_numpy.clip(0, 255)\n",
    "    image_numpy = image_numpy.astype(np.uint8)\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(filename)\n",
    "    print(\"Image saved as {}\".format(filename))\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.direction = direction\n",
    "        self.a_path = join(image_dir, \"a\")\n",
    "        self.b_path = join(image_dir, \"b\")\n",
    "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n",
    "\n",
    "        transform_list = [transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\n",
    "        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\n",
    "        a = a.resize((286, 286), Image.BICUBIC)\n",
    "        b = b.resize((286, 286), Image.BICUBIC)\n",
    "        a = transforms.ToTensor()(a)\n",
    "        b = transforms.ToTensor()(b)\n",
    "        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
    "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
    "    \n",
    "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
    "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
    "    \n",
    "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
    "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
    "            idx = torch.LongTensor(idx)\n",
    "            a = a.index_select(2, idx)\n",
    "            b = b.index_select(2, idx)\n",
    "\n",
    "        if self.direction == \"a2b\":\n",
    "            return a, b\n",
    "        else:\n",
    "            return b, a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "\n",
    "def get_training_set(root_dir, direction):\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "    return DatasetFromFolder(train_dir, direction)\n",
    "\n",
    "def get_validation_set(root_dir, direction):\n",
    "    val_dir = join(root_dir, \"val\")\n",
    "    return DatasetFromFolder(val_dir, direction)\n",
    "\n",
    "def get_testing_set(root_dir, direction):\n",
    "    test_dir = join(root_dir, \"test\")\n",
    "    return DatasetFromFolder(test_dir, direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "permanent-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "root_path='/scratch/datasets/pose2video/'\n",
    "train_set = get_training_set(root_path, 'a2b')\n",
    "train_data_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "twelve-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "root_path='/scratch/datasets/pose2video/'\n",
    "val_set = get_validation_set(root_path, 'a2b')\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comfortable-gates",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/abi/venv/ms/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/scratch/abi/venv/ms/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/scratch/abi/venv/ms/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "G = Generator(3, 64, 3)\n",
    "D = Discriminator(6, 64, 1)\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "G.normal_weight_init(mean=0.0, std=0.02)\n",
    "D.normal_weight_init(mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "atomic-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv5): ConvBlock(\n",
       "    (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function\n",
    "BCE_loss = torch.nn.BCELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "l1_lambda = 100\n",
    "\n",
    "# learning rate\n",
    "lrG = 0.0002\n",
    "lrD = 0.0002\n",
    "beta1 = 0.75\n",
    "beta2 = 0.999\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "\n",
    "G.train()\n",
    "D.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [14237/14237], D_loss: 0.7970, G_loss: 4.9504\n"
     ]
    }
   ],
   "source": [
    "# Training GAN\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # training\n",
    "    for i, (input, target) in enumerate(train_data_loader):\n",
    "        # input & target image data\n",
    "        x_ = Variable(input.cuda())\n",
    "        y_ = Variable(target.cuda())\n",
    "\n",
    "        # Train discriminator with real data\n",
    "        D_real_decision = D(x_, y_).squeeze()\n",
    "        real_ = Variable(torch.ones(D_real_decision.size()).cuda())\n",
    "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
    "\n",
    "        # Train discriminator with fake data\n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        fake_ = Variable(torch.zeros(D_fake_decision.size()).cuda())\n",
    "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
    "\n",
    "        # Back propagation\n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        G_fake_loss = BCE_loss(D_fake_decision, real_)\n",
    "\n",
    "        # L1 loss\n",
    "        l1_loss = l1_lambda * L1_loss(gen_image, y_)\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_fake_loss + l1_loss\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        # loss values\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "        \n",
    "    print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "          % (epoch+1, num_epochs, i+1, len(train_data_loader), D_loss, G_loss))\n",
    "\n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "    \n",
    "    # save checkpoints\n",
    "    checkpoint_dir = '/scratch/abi/MultiSign/ckpt/'\n",
    "    torch.save(G, checkpoint_dir + f'G-{epoch}-{G_loss}')\n",
    "    torch.save(D, checkpoint_dir + f'D-{epoch}-{D_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
