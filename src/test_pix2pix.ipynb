{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welsh-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from utils.dataset import SIGNUMDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import utils.skel as skel\n",
    "import deepdish as dd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "innocent-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, activation=True, batch_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.activation = activation\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2, True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation:\n",
    "            out = self.conv(self.lrelu(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            return self.bn(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class DeconvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, batch_norm=True, dropout=False):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "        self.drop = torch.nn.Dropout(0.5)\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(self.relu(x)))\n",
    "        else:\n",
    "            out = self.deconv(self.relu(x))\n",
    "\n",
    "        if self.dropout:\n",
    "            return self.drop(out)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv6 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv7 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv8 = ConvBlock(num_filter * 8, num_filter * 8, batch_norm=False)\n",
    "        # Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter * 8, num_filter * 8, dropout=True)\n",
    "        self.deconv2 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv3 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv4 = DeconvBlock(num_filter * 8 * 2, num_filter * 8)\n",
    "        self.deconv5 = DeconvBlock(num_filter * 8 * 2, num_filter * 4)\n",
    "        self.deconv6 = DeconvBlock(num_filter * 4 * 2, num_filter * 2)\n",
    "        self.deconv7 = DeconvBlock(num_filter * 2 * 2, num_filter)\n",
    "        self.deconv8 = DeconvBlock(num_filter * 2, output_dim, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(x)\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        enc4 = self.conv4(enc3)\n",
    "        enc5 = self.conv5(enc4)\n",
    "        enc6 = self.conv6(enc5)\n",
    "        enc7 = self.conv7(enc6)\n",
    "        enc8 = self.conv8(enc7)\n",
    "        # Decoder with skip-connections\n",
    "        dec1 = self.deconv1(enc8)\n",
    "        dec1 = torch.cat([dec1, enc7], 1)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc6], 1)\n",
    "        dec3 = self.deconv3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc5], 1)\n",
    "        dec4 = self.deconv4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc4], 1)\n",
    "        dec5 = self.deconv5(dec4)\n",
    "        dec5 = torch.cat([dec5, enc3], 1)\n",
    "        dec6 = self.deconv6(dec5)\n",
    "        dec6 = torch.cat([dec6, enc2], 1)\n",
    "        dec7 = self.deconv7(dec6)\n",
    "        dec7 = torch.cat([dec7, enc1], 1)\n",
    "        dec8 = self.deconv8(dec7)\n",
    "        out = torch.nn.Tanh()(dec8)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "            if isinstance(m, DeconvBlock):\n",
    "                torch.nn.init.normal(m.deconv.weight, mean, std)\n",
    "                \n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8, stride=1)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, output_dim, stride=1, batch_norm=False)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat([x, label], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        out = torch.nn.Sigmoid()(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ahead-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    img = img.resize((256, 256), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_img(image_tensor, filename):\n",
    "    image_numpy = image_tensor.float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    image_numpy = image_numpy.clip(0, 255)\n",
    "    image_numpy = image_numpy.astype(np.uint8)\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(filename)\n",
    "    print(\"Image saved as {}\".format(filename))\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.direction = direction\n",
    "        self.a_path = join(image_dir, \"a\")\n",
    "        self.b_path = join(image_dir, \"b\")\n",
    "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n",
    "\n",
    "        transform_list = [transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\n",
    "        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\n",
    "        a = a.resize((286, 286), Image.BICUBIC)\n",
    "        b = b.resize((286, 286), Image.BICUBIC)\n",
    "        a = transforms.ToTensor()(a)\n",
    "        b = transforms.ToTensor()(b)\n",
    "        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
    "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
    "    \n",
    "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
    "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
    "    \n",
    "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
    "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
    "            idx = torch.LongTensor(idx)\n",
    "            a = a.index_select(2, idx)\n",
    "            b = b.index_select(2, idx)\n",
    "\n",
    "        if self.direction == \"a2b\":\n",
    "            return a, b\n",
    "        else:\n",
    "            return b, a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "\n",
    "def get_training_set(root_dir, direction):\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "\n",
    "    return DatasetFromFolder(train_dir, direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "root_path='../data/'\n",
    "train_set = get_training_set(root_path, 'a2b')\n",
    "train_data_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atomic-index",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/abi/venv/ms/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/scratch/abi/venv/ms/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/scratch/abi/venv/ms/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "G = Generator(3, 64, 3)\n",
    "D = Discriminator(6, 64, 1)\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "G.normal_weight_init(mean=0.0, std=0.02)\n",
    "D.normal_weight_init(mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mental-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "BCE_loss = torch.nn.BCELoss().cuda()\n",
    "L1_loss = torch.nn.L1Loss().cuda()\n",
    "l1_lambda = 100\n",
    "\n",
    "# learning rate\n",
    "lrG = 0.0002\n",
    "lrD = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "northern-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training GAN\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # training\n",
    "    for i, (input, target) in enumerate(train_data_loader):\n",
    "        # only train on 1 sample for now\n",
    "        if i == 1:\n",
    "            break\n",
    "\n",
    "        # input & target image data\n",
    "        x_ = Variable(input.cuda())\n",
    "        y_ = Variable(target.cuda())\n",
    "\n",
    "        # Train discriminator with real data\n",
    "        D_real_decision = D(x_, y_).squeeze()\n",
    "        real_ = Variable(torch.ones(D_real_decision.size()).cuda())\n",
    "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
    "\n",
    "        # Train discriminator with fake data\n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        fake_ = Variable(torch.zeros(D_fake_decision.size()).cuda())\n",
    "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
    "\n",
    "        # Back propagation\n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        gen_image = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        G_fake_loss = BCE_loss(D_fake_decision, real_)\n",
    "\n",
    "        # L1 loss\n",
    "        l1_loss = l1_lambda * L1_loss(gen_image, y_)\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_fake_loss + l1_loss\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "#         # loss values\n",
    "#         D_losses.append(D_loss.data[0].item())\n",
    "#         G_losses.append(G_loss.data[0].item())\n",
    "\n",
    "#         print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "#               % (epoch+1, num_epochs, i+1, len(train_data_loader), D_loss.data[0], G_loss.data[0]))\n",
    "\n",
    "# ##         ============ TensorBoard logging ============#\n",
    "# #         D_logger.scalar_summary('losses', D_loss.data[0], step + 1)\n",
    "# #         G_logger.scalar_summary('losses', G_loss.data[0], step + 1)\n",
    "#         step += 1\n",
    "\n",
    "#     D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "#     G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "#     # avg loss values for plot\n",
    "#     D_avg_losses.append(D_avg_loss)\n",
    "#     G_avg_losses.append(G_avg_loss)\n",
    "\n",
    "#     # Show result for test image\n",
    "#     gen_image = G(Variable(test_input.cuda()))\n",
    "#     gen_image = gen_image.cpu().data\n",
    "#     utils.plot_test_result(test_input, test_target, gen_image, epoch, save=True, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "recognized-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confirmed-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_result(input, target, gen_image, epoch, training=True, save=False, save_dir='results/', show=False, fig_size=(5, 5)):\n",
    "    if not training:\n",
    "        fig_size = (input.size(2) * 3 / 100, input.size(3)/100)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=fig_size)\n",
    "    imgs = [input, gen_image, target]\n",
    "    for ax, img in zip(axes.flatten(), imgs):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box')\n",
    "        # Scale to 0-255\n",
    "        img = (((img[0] - img[0].min()) * 255) / (img[0].max() - img[0].min())).numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "        ax.imshow(img, cmap=None, aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    if training:\n",
    "        title = 'Epoch {0}'.format(epoch + 1)\n",
    "        fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        if training:\n",
    "            save_fn = save_dir + 'Result_epoch_{:d}'.format(epoch+1) + '.png'\n",
    "        else:\n",
    "            save_fn = save_dir + 'Test_result_{:d}'.format(epoch+1) + '.png'\n",
    "            fig.subplots_adjust(bottom=0)\n",
    "            fig.subplots_adjust(top=1)\n",
    "            fig.subplots_adjust(right=1)\n",
    "            fig.subplots_adjust(left=0)\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efficient-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 images are generated.\n",
      "2 images are generated.\n",
      "3 images are generated.\n",
      "4 images are generated.\n",
      "5 images are generated.\n",
      "6 images are generated.\n",
      "7 images are generated.\n",
      "8 images are generated.\n",
      "9 images are generated.\n",
      "10 images are generated.\n",
      "11 images are generated.\n",
      "12 images are generated.\n",
      "13 images are generated.\n",
      "14 images are generated.\n",
      "15 images are generated.\n",
      "16 images are generated.\n",
      "17 images are generated.\n",
      "18 images are generated.\n",
      "19 images are generated.\n",
      "20 images are generated.\n",
      "21 images are generated.\n",
      "22 images are generated.\n",
      "23 images are generated.\n",
      "24 images are generated.\n",
      "25 images are generated.\n",
      "26 images are generated.\n",
      "27 images are generated.\n",
      "28 images are generated.\n",
      "29 images are generated.\n",
      "30 images are generated.\n",
      "31 images are generated.\n",
      "32 images are generated.\n",
      "33 images are generated.\n",
      "34 images are generated.\n",
      "35 images are generated.\n",
      "36 images are generated.\n",
      "37 images are generated.\n",
      "38 images are generated.\n",
      "39 images are generated.\n",
      "40 images are generated.\n",
      "41 images are generated.\n",
      "42 images are generated.\n",
      "43 images are generated.\n",
      "44 images are generated.\n",
      "45 images are generated.\n",
      "46 images are generated.\n",
      "47 images are generated.\n",
      "48 images are generated.\n",
      "49 images are generated.\n",
      "50 images are generated.\n",
      "51 images are generated.\n",
      "52 images are generated.\n",
      "53 images are generated.\n",
      "54 images are generated.\n",
      "55 images are generated.\n",
      "56 images are generated.\n",
      "57 images are generated.\n",
      "58 images are generated.\n",
      "59 images are generated.\n",
      "60 images are generated.\n",
      "61 images are generated.\n",
      "62 images are generated.\n",
      "63 images are generated.\n",
      "64 images are generated.\n",
      "65 images are generated.\n",
      "66 images are generated.\n",
      "67 images are generated.\n",
      "68 images are generated.\n",
      "69 images are generated.\n",
      "70 images are generated.\n",
      "71 images are generated.\n",
      "72 images are generated.\n",
      "73 images are generated.\n",
      "74 images are generated.\n",
      "75 images are generated.\n",
      "76 images are generated.\n",
      "77 images are generated.\n",
      "78 images are generated.\n",
      "79 images are generated.\n",
      "80 images are generated.\n",
      "81 images are generated.\n",
      "82 images are generated.\n",
      "83 images are generated.\n",
      "84 images are generated.\n",
      "85 images are generated.\n",
      "86 images are generated.\n",
      "87 images are generated.\n",
      "88 images are generated.\n",
      "89 images are generated.\n",
      "90 images are generated.\n",
      "91 images are generated.\n",
      "92 images are generated.\n",
      "93 images are generated.\n",
      "94 images are generated.\n",
      "95 images are generated.\n",
      "96 images are generated.\n",
      "97 images are generated.\n",
      "98 images are generated.\n",
      "99 images are generated.\n",
      "100 images are generated.\n",
      "101 images are generated.\n",
      "102 images are generated.\n",
      "103 images are generated.\n",
      "104 images are generated.\n",
      "105 images are generated.\n",
      "106 images are generated.\n",
      "107 images are generated.\n",
      "108 images are generated.\n",
      "109 images are generated.\n",
      "110 images are generated.\n",
      "111 images are generated.\n",
      "112 images are generated.\n",
      "113 images are generated.\n",
      "114 images are generated.\n",
      "115 images are generated.\n",
      "116 images are generated.\n",
      "117 images are generated.\n",
      "118 images are generated.\n",
      "119 images are generated.\n"
     ]
    }
   ],
   "source": [
    "for i, (input, target) in enumerate(train_data_loader):\n",
    "    # input & target image data\n",
    "    x_ = Variable(input.cuda())\n",
    "    y_ = Variable(target.cuda())\n",
    "\n",
    "    gen_image = G(x_)\n",
    "    gen_image = gen_image.cpu().data\n",
    "\n",
    "    # Show result for test data\n",
    "    plot_test_result(input, target, gen_image, i, training=False, save=True, save_dir='/scratch/abi/MultiSign/test/')\n",
    "\n",
    "    print('%d images are generated.' % (i + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
