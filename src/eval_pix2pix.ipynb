{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "roman-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twelve-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    img = img.resize((256, 256), Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_img(image_tensor, filename):\n",
    "    image_numpy = image_tensor.float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    image_numpy = image_numpy.clip(0, 255)\n",
    "    image_numpy = image_numpy.astype(np.uint8)\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(filename)\n",
    "    print(\"Image saved as {}\".format(filename))\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, direction):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.direction = direction\n",
    "        self.a_path = join(image_dir, \"a\")\n",
    "        self.b_path = join(image_dir, \"b\")\n",
    "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n",
    "\n",
    "        transform_list = [transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\n",
    "        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\n",
    "        a = a.resize((286, 286), Image.BICUBIC)\n",
    "        b = b.resize((286, 286), Image.BICUBIC)\n",
    "        a = transforms.ToTensor()(a)\n",
    "        b = transforms.ToTensor()(b)\n",
    "        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
    "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
    "    \n",
    "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
    "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
    "    \n",
    "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
    "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
    "            idx = torch.LongTensor(idx)\n",
    "            a = a.index_select(2, idx)\n",
    "            b = b.index_select(2, idx)\n",
    "\n",
    "        if self.direction == \"a2b\":\n",
    "            return a, b\n",
    "        else:\n",
    "            return b, a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "\n",
    "def get_training_set(root_dir, direction):\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "    return DatasetFromFolder(train_dir, direction)\n",
    "\n",
    "def get_validation_set(root_dir, direction):\n",
    "    val_dir = join(root_dir, \"val\")\n",
    "    return DatasetFromFolder(val_dir, direction)\n",
    "\n",
    "def get_testing_set(root_dir, direction):\n",
    "    test_dir = join(root_dir, \"test\")\n",
    "    return DatasetFromFolder(test_dir, direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "root_path='/scratch/datasets/pose2video/'\n",
    "test_set = get_testing_set(root_path, 'a2b')\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=4, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "circular-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_result(input, target, gen_image, epoch, training=True, save=False, save_dir='results/', show=False, fig_size=(5, 5)):\n",
    "    if not training:\n",
    "        fig_size = (input.size(2) * 3 / 100, input.size(3)/100)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=fig_size)\n",
    "    imgs = [input, gen_image, target]\n",
    "    for ax, img in zip(axes.flatten(), imgs):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box')\n",
    "        # Scale to 0-255\n",
    "        img = (((img[0] - img[0].min()) * 255) / (img[0].max() - img[0].min())).numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "        ax.imshow(img, cmap=None, aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    if training:\n",
    "        title = 'Epoch {0}'.format(epoch + 1)\n",
    "        fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        if training:\n",
    "            save_fn = save_dir + 'Result_epoch_{:d}'.format(epoch+1) + '.png'\n",
    "        else:\n",
    "            save_fn = save_dir + 'Test_result_{:d}'.format(epoch+1) + '.png'\n",
    "            fig.subplots_adjust(bottom=0)\n",
    "            fig.subplots_adjust(top=1)\n",
    "            fig.subplots_adjust(right=1)\n",
    "            fig.subplots_adjust(left=0)\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "better-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, activation=True, batch_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.activation = activation\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2, True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation:\n",
    "            out = self.conv(self.lrelu(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            return self.bn(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class DeconvBlock(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, batch_norm=True, dropout=False):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.bn = torch.nn.BatchNorm2d(output_size)\n",
    "        self.drop = torch.nn.Dropout(0.5)\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(self.relu(x)))\n",
    "        else:\n",
    "            out = self.deconv(self.relu(x))\n",
    "\n",
    "        if self.dropout:\n",
    "            return self.drop(out)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv6 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv7 = ConvBlock(num_filter * 8, num_filter * 8)\n",
    "        self.conv8 = ConvBlock(num_filter * 8, num_filter * 8, batch_norm=False)\n",
    "        # Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter * 8, num_filter * 8, dropout=True)\n",
    "        self.deconv2 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv3 = DeconvBlock(num_filter * 8 * 2, num_filter * 8, dropout=True)\n",
    "        self.deconv4 = DeconvBlock(num_filter * 8 * 2, num_filter * 8)\n",
    "        self.deconv5 = DeconvBlock(num_filter * 8 * 2, num_filter * 4)\n",
    "        self.deconv6 = DeconvBlock(num_filter * 4 * 2, num_filter * 2)\n",
    "        self.deconv7 = DeconvBlock(num_filter * 2 * 2, num_filter)\n",
    "        self.deconv8 = DeconvBlock(num_filter * 2, output_dim, batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(x)\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        enc4 = self.conv4(enc3)\n",
    "        enc5 = self.conv5(enc4)\n",
    "        enc6 = self.conv6(enc5)\n",
    "        enc7 = self.conv7(enc6)\n",
    "        enc8 = self.conv8(enc7)\n",
    "        # Decoder with skip-connections\n",
    "        dec1 = self.deconv1(enc8)\n",
    "        dec1 = torch.cat([dec1, enc7], 1)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc6], 1)\n",
    "        dec3 = self.deconv3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc5], 1)\n",
    "        dec4 = self.deconv4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc4], 1)\n",
    "        dec5 = self.deconv5(dec4)\n",
    "        dec5 = torch.cat([dec5, enc3], 1)\n",
    "        dec6 = self.deconv6(dec5)\n",
    "        dec6 = torch.cat([dec6, enc2], 1)\n",
    "        dec7 = self.deconv7(dec6)\n",
    "        dec7 = torch.cat([dec7, enc1], 1)\n",
    "        dec8 = self.deconv8(dec7)\n",
    "        out = torch.nn.Tanh()(dec8)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "            if isinstance(m, DeconvBlock):\n",
    "                torch.nn.init.normal(m.deconv.weight, mean, std)\n",
    "                \n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, activation=False, batch_norm=False)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        self.conv4 = ConvBlock(num_filter * 4, num_filter * 8, stride=1)\n",
    "        self.conv5 = ConvBlock(num_filter * 8, output_dim, stride=1, batch_norm=False)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = torch.cat([x, label], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        out = torch.nn.Sigmoid()(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dietary-appendix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv5): ConvBlock(\n",
       "    (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = torch.load('/scratch/abi/MultiSign/ckpt/G-50-13.26486587524414')\n",
    "D = torch.load('/scratch/abi/MultiSign/ckpt/D-50-0.06963573396205902')\n",
    "G.cuda()\n",
    "D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polyphonic-expansion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): ConvBlock(\n",
       "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv5): ConvBlock(\n",
       "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv6): ConvBlock(\n",
       "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv7): ConvBlock(\n",
       "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv8): ConvBlock(\n",
       "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (deconv1): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv2): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv3): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv4): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv5): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv6): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv7): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv8): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominican-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 images are generated.\n"
     ]
    }
   ],
   "source": [
    "for i, (input, target) in enumerate(test_data_loader):\n",
    "    # input & target image data\n",
    "    x_ = Variable(input.cuda())\n",
    "    y_ = Variable(target.cuda())\n",
    "\n",
    "    gen_image = G(x_)\n",
    "    gen_image = gen_image.cpu().data\n",
    "\n",
    "    # Show result for test data\n",
    "    plot_test_result(input, target, gen_image, i, training=False, save=True, save_dir='/scratch/abi/MultiSign/test/')\n",
    "\n",
    "    print('%d images are generated.' % (i + 1))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
